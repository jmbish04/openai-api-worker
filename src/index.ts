/**
 * OpenAI-Compatible API Worker (TypeScript Version)
 *
 * This worker provides a unified, OpenAI-compatible API endpoint for various AI models,
 * including those from Cloudflare AI, OpenAI, and Google Gemini. It intelligently
 * routes requests to the appropriate backend provider based on the requested model.
 *
 * @version 2.1.0
 * @author Colby
 */

import OpenAI from 'openai';
import { GoogleGenAI } from '@google/genai';
import type { ChatCompletion } from 'openai/resources/chat/completions';

// --- Type Definitions ---

// Environment interface is generated by wrangler types
// The Env interface will be available from worker-configuration.d.ts

/**
 * Represents the structure of a chat message in an OpenAI API request.
 */
interface ChatMessage {
	role: 'system' | 'user' | 'assistant';
	content: string;
}

/**
 * Represents the body of a request to the /v1/chat/completions endpoint.
 */
interface ChatCompletionRequestBody {
	model?: string;
	messages: ChatMessage[];
	stream?: boolean;
	max_tokens?: number;
	temperature?: number;
	top_p?: number;
	frequency_penalty?: number;
	presence_penalty?: number;
	response_format?: {
		type: 'text' | 'json_object' | 'json_schema';
		schema?: any;
	};
	memory_keyword?: string; // Optional keyword for KV memory isolation
	[key: string]: any; // Allow other parameters
}

/**
 * Represents the body of a request to the legacy /v1/completions endpoint.
 */
interface CompletionRequestBody {
    model?: string;
    prompt: string;
    max_tokens?: number;
    temperature?: number;
    stream?: boolean;
}


/**
 * Represents a model available from any provider.
 */
interface ApiModel {
    id: string;
    object: 'model';
    owner: string;
    created?: number;
}

type Provider = 'cloudflare' | 'openai' | 'gemini';
type ModelType = 'llama4' | 'llama' | 'openai' | 'gemini' | 'input';

/**
 * Cloudflare AI model information from the core API
 */
interface CloudflareAIModel {
	id: string;
	source: number;
	name: string;
	description: string;
	task: {
		id: string;
		name: string;
		description: string;
	};
	created_at: string;
	tags: string[];
	properties: Array<{
		property_id: string;
		value: any;
	}>;
}

/**
 * Cloudflare AI models response structure
 */
interface CloudflareAIModelsResponse {
	providers: Record<string, CloudflareAIModel[]>;
}

// --- Utility Functions ---

/**
 * Logs a debug message if debug logging is enabled.
 * @param env - The worker's environment variables.
 * @param message - The message to log.
 * @param data - Optional data to log as an object.
 */
function debugLog(env: Env, message: string, data: Record<string, any> | null = null): void {
	if (env.DEBUG_LOGGING === 'true') {
		const timestamp = new Date().toISOString();
		if (data) {
			console.log(`[${timestamp}] ${message}:`, JSON.stringify(data, null, 2));
		} else {
			console.log(`[${timestamp}] ${message}`);
		}
	}
}

/**
 * Logs an error message to the console.
 * @param message - The error message.
 * @param error - Optional error object or details.
 */
function errorLog(message: string, error: any | null = null): void {
	const timestamp = new Date().toISOString();
	if (error) {
		console.error(`[${timestamp}] ERROR: ${message}:`, error instanceof Error ? error.stack : JSON.stringify(error));
	} else {
		console.error(`[${timestamp}] ERROR: ${message}`);
	}
}


// --- Main Worker ---

export default {
	async fetch(request: Request, env: Env): Promise<Response> {
		const url = new URL(request.url);
		const path = url.pathname;

		debugLog(env, `Incoming request: ${request.method} ${path}`);

		const corsHeaders: Record<string, string> = {
			'Access-Control-Allow-Origin': '*',
			'Access-Control-Allow-Methods': 'GET, POST, PUT, DELETE, OPTIONS',
			'Access-Control-Allow-Headers': 'Content-Type, Authorization',
		};

		if (request.method === 'OPTIONS') {
			debugLog(env, 'Handling CORS preflight request');
			return new Response(null, { headers: corsHeaders });
		}

		try {
			// --- Static Asset and Public Routes (No Auth Required) ---
            const publicRoutes: Record<string, string> = {
                '/': '/index.html',
                '/index.html': '/index.html',
                '/openapi.json': '/openapi.json',
                '/test-dropdowns.html': '/test-dropdowns.html',
                '/debug-test.html': '/debug-test.html',
                '/quick-test.html': '/quick-test.html',
                '/cloudflare_ai_models.json': '/cloudflare_ai_models.json',
            };

            if (publicRoutes[path]) {
                debugLog(env, `Serving static asset: ${publicRoutes[path]}`);
                try {
                    const asset = await env.ASSETS.fetch(new URL(publicRoutes[path], request.url));
                    const contentType = path.endsWith('.json') ? 'application/json' : 'text/html';
                    return new Response(await asset.text(), {
                        headers: { 'Content-Type': contentType, ...corsHeaders },
                    });
                } catch (error) {
                    errorLog(`Error serving static asset: ${path}`, error);
                    return new Response(`Not Found: ${path}`, { status: 404, headers: corsHeaders });
                }
            }


			if (path === '/health') {
				debugLog(env, 'Health check requested');
				return new Response(JSON.stringify({
					status: 'healthy',
					service: 'openai-api-worker',
					timestamp: new Date().toISOString(),
					version: '2.1.0',
					providers: {
						cloudflare: true,
						openai: !!env.OPENAI_API_KEY,
						gemini: !!env.GEMINI_API_KEY
					}
				}), { headers: { 'Content-Type': 'application/json', ...corsHeaders } });
			}

			// --- API Endpoint Authentication ---
			const authResult = await authenticateRequest(request, env);
			if (!authResult.success) {
				errorLog(`Authentication failed: ${authResult.error}`);
				return new Response(JSON.stringify({ error: { message: authResult.error, type: 'invalid_request_error' } }), {
					status: 401,
					headers: { 'Content-Type': 'application/json', ...corsHeaders },
				});
			}
			debugLog(env, 'Authentication successful');

			// --- API Endpoint Routing ---
			if (path === '/v1/chat/completions') {
				return handleChatCompletions(request, env, corsHeaders);
			}
			if (path === '/v1/chat/completions/structured') {
				return handleStructuredChatCompletions(request, env, corsHeaders);
			}
			if (path === '/v1/chat/completions/text') {
				return handleTextChatCompletions(request, env, corsHeaders);
			}
			if (path === '/v1/models') {
				return handleModelsRequest(request, env, corsHeaders);
			}
			if (path === '/v1/completions') {
				return handleCompletions(request, env, corsHeaders);
			}
			if (path === '/test/apis') {
				return handleTestAPIs(request, env, corsHeaders);
			}

			// --- Not Found ---
			return new Response(JSON.stringify({ error: { message: 'Not Found', type: 'invalid_request_error' } }), {
				status: 404,
				headers: { 'Content-Type': 'application/json', ...corsHeaders },
			});

		} catch (error) {
			errorLog('Unhandled worker error', error);
			const errorResponse = {
				error: {
					message: 'Internal Server Error',
					type: 'server_error',
					details: error instanceof Error ? error.message : 'Unknown error',
					request_id: generateId()
				}
			};
			return new Response(JSON.stringify(errorResponse), {
				status: 500,
				headers: { 'Content-Type': 'application/json', ...corsHeaders },
			});
		}
	},
};

// --- Core Logic ---

/**
 * Adds memory context from KV storage to messages using keyword-based isolation
 */
async function addMemoryContext(messages: ChatMessage[], memoryKeyword: string | null, env: Env): Promise<ChatMessage[]> {
    try {
        // If no memory keyword provided, return original messages
        if (!memoryKeyword) return messages;

        // Get the last user message to extract context
        const lastUserMessage = messages.filter(m => m.role === 'user').pop();
        if (!lastUserMessage) return messages;

        // Create keyword-based context key
        const contextKey = `memory:${memoryKeyword}:${Buffer.from(lastUserMessage.content).toString('base64').slice(0, 20)}`;
        
        // Try to get relevant memory from KV
        const memoryData = await env.AI_MEMORY.get(contextKey);
        if (!memoryData) return messages;

        const memory = JSON.parse(memoryData);
        
        // Add memory context to the system message or create one
        const systemMessage: ChatMessage = {
            role: 'system',
            content: `Previous context (${memoryKeyword}): ${memory.context || 'No previous context available.'}`
        };

        // Insert system message at the beginning
        return [systemMessage, ...messages];
    } catch (error) {
        debugLog(env, 'Error adding memory context', { error: error instanceof Error ? error.message : String(error) });
        return messages; // Return original messages if memory fails
    }
}

/**
 * Saves conversation context to KV memory using keyword-based isolation
 */
async function saveMemoryContext(messages: ChatMessage[], response: string, memoryKeyword: string | null, env: Env): Promise<void> {
    try {
        // If no memory keyword provided, don't save memory
        if (!memoryKeyword) return;

        const lastUserMessage = messages.filter(m => m.role === 'user').pop();
        if (!lastUserMessage) return;

        const contextKey = `memory:${memoryKeyword}:${Buffer.from(lastUserMessage.content).toString('base64').slice(0, 20)}`;
        const memoryData = {
            context: `${lastUserMessage.content}\n\nResponse: ${response}`,
            timestamp: Date.now(),
            model: 'ai-conversation',
            keyword: memoryKeyword
        };

        await env.AI_MEMORY.put(contextKey, JSON.stringify(memoryData), {
            expirationTtl: 86400 // 24 hours
        });
    } catch (error) {
        debugLog(env, 'Error saving memory context', { error: error instanceof Error ? error.message : String(error) });
    }
}

/**
 * Searches for memories by keyword
 */
async function searchMemories(memoryKeyword: string, query: string, env: Env): Promise<string[]> {
    try {
        if (!memoryKeyword) return [];

        // Get all keys for this memory keyword
        const listResult = await env.AI_MEMORY.list({ prefix: `memory:${memoryKeyword}:` });
        
        const memories: string[] = [];
        for (const key of listResult.keys) {
            const memoryData = await env.AI_MEMORY.get(key.name);
            if (memoryData) {
                const memory = JSON.parse(memoryData);
                if (memory.context && memory.context.toLowerCase().includes(query.toLowerCase())) {
                    memories.push(memory.context);
                }
            }
        }
        
        return memories;
    } catch (error) {
        debugLog(env, 'Error searching memories', { error: error instanceof Error ? error.message : String(error) });
        return [];
    }
}

/**
 * Clears all memories for a specific keyword
 */
async function clearMemories(memoryKeyword: string, env: Env): Promise<boolean> {
    try {
        if (!memoryKeyword) return false;

        // Get all keys for this memory keyword
        const listResult = await env.AI_MEMORY.list({ prefix: `memory:${memoryKeyword}:` });
        
        // Delete all memories for this keyword
        for (const key of listResult.keys) {
            await env.AI_MEMORY.delete(key.name);
        }
        
        return true;
    } catch (error) {
        debugLog(env, 'Error clearing memories', { error: error instanceof Error ? error.message : String(error) });
        return false;
    }
}

// Export these functions for potential future use
export { searchMemories, clearMemories };

async function authenticateRequest(request: Request, env: Env): Promise<{ success: boolean; error?: string }> {
	const authHeader = request.headers.get('Authorization');
	if (!authHeader) {
		return { success: false, error: 'Missing Authorization header' };
	}
	if (!authHeader.startsWith('Bearer ')) {
		return { success: false, error: 'Invalid Authorization header format. Use: Bearer <token>' };
	}
	const token = authHeader.replace('Bearer ', '');

	if (env.DEBUG_LOGGING === 'true' || !env.WORKER_API_KEY) {
		debugLog(env, 'Development mode: bypassing API key validation');
		return { success: true };
	}

	if (token !== env.WORKER_API_KEY) {
		debugLog(env, 'API key validation failed');
		return { success: false, error: 'Invalid API key' };
	}

	return { success: true };
}

async function handleChatCompletions(request: Request, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        const body = await request.json() as ChatCompletionRequestBody;
        const {
            model = env.DEFAULT_MODEL || '@cf/meta/llama-4-scout-17b-16e-instruct',
            messages,
            stream = false,
            max_tokens = 2048,
            temperature = 0.7,
            top_p = 1,
            frequency_penalty = 0,
            presence_penalty = 0,
            response_format,
            memory_keyword,
            ...otherParams
        } = body;

        if (!messages || !Array.isArray(messages)) {
            return new Response(JSON.stringify({ error: { message: '`messages` parameter is required and must be an array', type: 'invalid_request_error' } }), {
                status: 400,
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }
        
        const provider = detectProvider(model, env);
        const mappedModel = mapModelName(model, provider, env);
        const modelType = getModelType(mappedModel, provider);
        debugLog(env, `Routing to provider`, { provider, model: mappedModel, type: modelType });

        // Add memory context if keyword is provided
        const messagesWithMemory = memory_keyword ? await addMemoryContext(messages, memory_keyword, env) : messages;

        switch (provider) {
            case 'openai':
                return handleOpenAIRequest({ messages: messagesWithMemory, model: mappedModel, stream, max_tokens, temperature, top_p, frequency_penalty, presence_penalty, response_format, memory_keyword, ...otherParams }, env, corsHeaders);
            case 'gemini':
                return handleGeminiRequest({ messages: messagesWithMemory, model: mappedModel, stream, max_tokens, temperature, response_format, memory_keyword }, env, corsHeaders);
            case 'cloudflare':
            default:
                return handleCloudflareRequest({ messages: messagesWithMemory, model: mappedModel, modelType, stream, max_tokens, temperature, top_p, originalModel: model, response_format, memory_keyword }, env, corsHeaders);
        }
    } catch (error) {
        errorLog('Chat completions error', error);
        return new Response(JSON.stringify({ error: { message: 'AI service error', type: 'server_error', details: (error as Error).message } }), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

async function handleStructuredChatCompletions(request: Request, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        const body = await request.json() as ChatCompletionRequestBody;
        const {
            model = env.DEFAULT_MODEL || '@cf/meta/llama-4-scout-17b-16e-instruct',
            messages,
            stream = false,
            max_tokens = 2048,
            temperature = 0.7,
            top_p = 1,
            frequency_penalty = 0,
            presence_penalty = 0,
            response_format,
            memory_keyword,
            ...otherParams
        } = body;

        if (!messages || !Array.isArray(messages)) {
            return new Response(JSON.stringify({ error: { message: '`messages` parameter is required and must be an array', type: 'invalid_request_error' } }), {
                status: 400,
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }

        if (!response_format || !response_format.schema) {
            return new Response(JSON.stringify({ error: { message: '`response_format.schema` is required for structured responses', type: 'invalid_request_error' } }), {
                status: 400,
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }
        
        const provider = detectProvider(model, env);
        const mappedModel = mapModelName(model, provider, env);
        const modelType = getModelType(mappedModel, provider);
        debugLog(env, `Routing structured request to provider`, { provider, model: mappedModel, type: modelType });

        // Add KV memory context to messages if keyword is provided
        const messagesWithMemory = memory_keyword ? await addMemoryContext(messages, memory_keyword, env) : messages;

        switch (provider) {
            case 'openai':
                return handleOpenAIStructuredRequest({ messages: messagesWithMemory, model: mappedModel, stream, max_tokens, temperature, top_p, frequency_penalty, presence_penalty, response_format, memory_keyword, ...otherParams }, env, corsHeaders);
            case 'gemini':
                return handleGeminiStructuredRequest({ messages: messagesWithMemory, model: mappedModel, stream, max_tokens, temperature, response_format, memory_keyword }, env, corsHeaders);
            case 'cloudflare':
            default:
                return handleCloudflareStructuredRequest({ messages: messagesWithMemory, model: mappedModel, modelType, stream, max_tokens, temperature, top_p, originalModel: model, response_format, memory_keyword }, env, corsHeaders);
        }
    } catch (error) {
        errorLog('Structured chat completions error', error);
        return new Response(JSON.stringify({ error: { message: 'AI service error', type: 'server_error', details: (error as Error).message } }), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

async function handleTextChatCompletions(request: Request, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        const body = await request.json() as ChatCompletionRequestBody;
        const {
            model = env.DEFAULT_MODEL || '@cf/meta/llama-4-scout-17b-16e-instruct',
            messages,
            stream = false,
            max_tokens = 2048,
            temperature = 0.7,
            top_p = 1,
            frequency_penalty = 0,
            presence_penalty = 0,
            memory_keyword,
            ...otherParams
        } = body;

        if (!messages || !Array.isArray(messages)) {
            return new Response(JSON.stringify({ error: { message: '`messages` parameter is required and must be an array', type: 'invalid_request_error' } }), {
                status: 400,
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }
        
        const provider = detectProvider(model, env);
        const mappedModel = mapModelName(model, provider, env);
        const modelType = getModelType(mappedModel, provider);
        debugLog(env, `Routing text request to provider`, { provider, model: mappedModel, type: modelType });

        // Add KV memory context to messages if keyword is provided
        const messagesWithMemory = memory_keyword ? await addMemoryContext(messages, memory_keyword, env) : messages;

        switch (provider) {
            case 'openai':
                return handleOpenAITextRequest({ messages: messagesWithMemory, model: mappedModel, stream, max_tokens, temperature, top_p, frequency_penalty, presence_penalty, memory_keyword, ...otherParams }, env, corsHeaders);
            case 'gemini':
                return handleGeminiTextRequest({ messages: messagesWithMemory, model: mappedModel, stream, max_tokens, temperature, memory_keyword }, env, corsHeaders);
            case 'cloudflare':
            default:
                return handleCloudflareTextRequest({ messages: messagesWithMemory, model: mappedModel, modelType, stream, max_tokens, temperature, top_p, originalModel: model, memory_keyword }, env, corsHeaders);
        }
    } catch (error) {
        errorLog('Text chat completions error', error);
        return new Response(JSON.stringify({ error: { message: 'AI service error', type: 'server_error', details: (error as Error).message } }), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

// --- Provider Handlers ---

// --- Structured Response Handlers ---

async function handleOpenAIStructuredRequest(params: any, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        if (!env.OPENAI_API_KEY) {
            throw new Error('OpenAI API key not configured');
        }
        
        debugLog(env, 'Making OpenAI structured request', { model: params.model });
        
        const openai = new OpenAI({ apiKey: env.OPENAI_API_KEY });
        
        // Prepare request parameters with structured response support
        const requestParams = { ...params };
        
        // Ensure response_format is properly structured for OpenAI
        if (params.response_format) {
            if (params.response_format.type === 'json_object') {
                requestParams.response_format = { type: 'json_object' };
            } else if (params.response_format.type === 'json_schema' && params.response_format.schema) {
                // Ensure the schema has additionalProperties: false for OpenAI
                const schema = addAdditionalPropertiesFalse(params.response_format.schema);
                
                requestParams.response_format = {
                    type: 'json_schema',
                    json_schema: {
                        name: 'response_schema',
                        strict: true,
                        schema: schema
                    }
                };
            }
        }
        
        const completion = await openai.chat.completions.create(requestParams);

        if (params.stream) {
            // For streaming, return the completion as a readable stream
            return new Response(completion as any, {
                headers: { 'Content-Type': 'text/event-stream', ...corsHeaders }
            });
        } else {
            // Save memory context
            const responseText = completion.choices[0]?.message?.content || '';
            await saveMemoryContext(params.messages, responseText, params.memory_keyword, env);
            
            return new Response(JSON.stringify(completion), {
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }
    } catch (error) {
        errorLog('OpenAI structured request failed', error);
        const errorResponse = {
            error: {
                message: 'OpenAI API error',
                type: 'api_error',
                details: error instanceof Error ? error.message : 'Unknown OpenAI error',
                request_id: generateId()
            }
        };
        return new Response(JSON.stringify(errorResponse), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

async function handleGeminiStructuredRequest(params: any, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        if (!env.GEMINI_API_KEY) {
            throw new Error('Gemini API key not configured');
        }
        
        debugLog(env, 'Making Gemini structured request', { model: params.model });

        const genAI = new GoogleGenAI({ apiKey: env.GEMINI_API_KEY });
        
        // Convert messages to Gemini format
        const conversationMessages = params.messages.filter((msg: ChatMessage) => msg.role !== 'system');
        
        const history = conversationMessages.map((msg: ChatMessage) => ({
            role: msg.role === 'assistant' ? 'model' : 'user',
            parts: [{ text: msg.content }]
        }));

        // Prepare config for structured response
        let config: any = {};
        if (params.response_format) {
            if (params.response_format.type === 'json_object') {
                config.responseMimeType = 'application/json';
            } else if (params.response_format.type === 'json_schema' && params.response_format.schema) {
                config.responseMimeType = 'application/json';
                config.responseSchema = convertOpenAISchemaToGemini(params.response_format.schema);
            }
        }

        if (params.stream) {
            const { readable, writable } = new TransformStream();
            const writer = writable.getWriter();
            const encoder = new TextEncoder();

            const streamResponse = async () => {
                try {
                    const stream = await genAI.models.generateContentStream({
                        model: params.model,
                        contents: history,
                        config: Object.keys(config).length > 0 ? config : undefined
                    });

                    let fullText = '';
                    for await (const chunk of stream) {
                        const chunkText = chunk.text;
                        if (chunkText) {
                            fullText += chunkText;
                            
                            // Create OpenAI-compatible streaming response
                            const delta = {
                                id: `chatcmpl-${generateId()}`,
                                object: 'chat.completion.chunk',
                                created: Math.floor(Date.now() / 1000),
                                model: params.model,
                                choices: [{
                                    index: 0,
                                    delta: { content: chunkText },
                                    finish_reason: null
                                }]
                            };
                            
                            await writer.write(encoder.encode(`data: ${JSON.stringify(delta)}\n\n`));
                        }
                    }

                    // Send final chunk
                    const finalChunk = {
                        id: `chatcmpl-${generateId()}`,
                        object: 'chat.completion.chunk',
                        created: Math.floor(Date.now() / 1000),
                        model: params.model,
                        choices: [{
                            index: 0,
                            delta: {},
                            finish_reason: 'stop'
                        }]
                    };
                    
                    await writer.write(encoder.encode(`data: ${JSON.stringify(finalChunk)}\n\n`));
                    await writer.write(encoder.encode('data: [DONE]\n\n'));
                } catch (error) {
                    errorLog('Gemini streaming error', error);
                    const errorChunk = `data: ${JSON.stringify({ error: { message: (error as Error).message, type: 'server_error' } })}\n\n`;
                    await writer.write(encoder.encode(errorChunk));
                } finally {
                    writer.close();
                }
            };

            streamResponse();
            return new Response(readable, {
                headers: { 'Content-Type': 'text/event-stream', ...corsHeaders }
            });
        } else {
            const result = await genAI.models.generateContent({
                model: params.model,
                contents: history,
                config: Object.keys(config).length > 0 ? config : undefined
            });
            const text = result.text || '';

            // Save memory context
            await saveMemoryContext(params.messages, text, params.memory_keyword, env);

            const openaiResponse: ChatCompletion = {
                id: `chatcmpl-${generateId()}`,
                object: 'chat.completion',
                created: Math.floor(Date.now() / 1000),
                model: params.model,
                choices: [{ 
                    index: 0, 
                    message: { role: 'assistant', content: text, refusal: null }, 
                    finish_reason: 'stop',
                    logprobs: null
                }],
                usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 } // Usage not provided by Gemini
            };

            return new Response(JSON.stringify(openaiResponse), {
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }
    } catch (error) {
        errorLog('Gemini structured request failed', error);
        const errorResponse = {
            error: {
                message: 'Gemini API error',
                type: 'api_error',
                details: error instanceof Error ? error.message : 'Unknown Gemini error',
                request_id: generateId()
            }
        };
        return new Response(JSON.stringify(errorResponse), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

async function handleCloudflareStructuredRequest(params: any, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        debugLog(env, `Making Cloudflare structured request`, { model: params.model, type: params.modelType });
        const convertedMessages = convertMessages(params.messages, 'cloudflare', params.modelType);
    
        let aiRequestPayload: any;
        if (params.modelType === 'llama4') {
            aiRequestPayload = { 
                messages: Array.isArray(convertedMessages) ? convertedMessages : params.messages, 
                stream: params.stream 
            };
            
            // Add structured output support for advanced models
            if (params.response_format) {
                if (params.response_format.type === 'json_object') {
                    aiRequestPayload.response_format = { type: 'json_object' };
                } else if (params.response_format.type === 'json_schema' && params.response_format.schema) {
                    aiRequestPayload.response_format = { 
                        type: 'json_schema',
                        json_schema: params.response_format.schema
                    };
                }
            }
        } else {
            aiRequestPayload = { 
                input: (convertedMessages as {input: string}).input, 
                stream: params.stream 
            };
        }
        
        const responseStream: ReadableStream = await env.AI.run(params.model, aiRequestPayload);

        if (params.stream) {
            const { readable, writable } = new TransformStream();
            const writer = writable.getWriter();
            const encoder = new TextEncoder();

            const streamResponse = async () => {
                const reader = responseStream.getReader();
                try {
                    while (true) {
                        const { done, value } = await reader.read();
                        if (done) break;
                        // The value from env.AI.run stream is already a server-sent event string
                        // We just need to pass it through.
                        await writer.write(value);
                    }
                    const finalChunk = `data: [DONE]\n\n`;
                    await writer.write(encoder.encode(finalChunk));
                } catch (error) {
                    errorLog('Cloudflare streaming error', error);
                    const errorChunk = `data: ${JSON.stringify({ error: { message: (error as Error).message, type: 'server_error' } })}\n\n`;
                    await writer.write(encoder.encode(errorChunk));
                } finally {
                    writer.close();
                }
            };

            streamResponse();
            return new Response(readable, {
                headers: { 'Content-Type': 'text/event-stream', ...corsHeaders }
            });
        } else {
            const reader = responseStream.getReader();
            const chunks: Uint8Array[] = [];
            
            while (true) {
                const { done, value } = await reader.read();
                if (done) break;
                chunks.push(value);
            }
            
            const fullResponse = new TextDecoder().decode(new Uint8Array(chunks.reduce((acc, chunk) => new Uint8Array([...acc, ...chunk]), new Uint8Array(0))));
            
            // Save memory context
            await saveMemoryContext(params.messages, fullResponse, params.memory_keyword, env);
            
            return new Response(fullResponse, {
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }
    } catch (error) {
        errorLog('Cloudflare structured request failed', error);
        const errorResponse = {
            error: {
                message: 'Cloudflare AI error',
                type: 'api_error',
                details: error instanceof Error ? error.message : 'Unknown Cloudflare AI error',
                request_id: generateId()
            }
        };
        return new Response(JSON.stringify(errorResponse), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

// --- Text Generation Handlers ---

async function handleOpenAITextRequest(params: any, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        if (!env.OPENAI_API_KEY) {
            throw new Error('OpenAI API key not configured');
        }
        
        debugLog(env, 'Making OpenAI text request', { model: params.model });
        
        const openai = new OpenAI({ apiKey: env.OPENAI_API_KEY });
        const completion = await openai.chat.completions.create(params);

        if (params.stream) {
            // For streaming, return the completion as a readable stream
            return new Response(completion as any, {
                headers: { 'Content-Type': 'text/event-stream', ...corsHeaders }
            });
        } else {
            // Save memory context
            const responseText = completion.choices[0]?.message?.content || '';
            await saveMemoryContext(params.messages, responseText, params.memory_keyword, env);
            
            return new Response(JSON.stringify(completion), {
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }
    } catch (error) {
        errorLog('OpenAI text request failed', error);
        const errorResponse = {
            error: {
                message: 'OpenAI API error',
                type: 'api_error',
                details: error instanceof Error ? error.message : 'Unknown OpenAI error',
                request_id: generateId()
            }
        };
        return new Response(JSON.stringify(errorResponse), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

async function handleGeminiTextRequest(params: any, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        if (!env.GEMINI_API_KEY) {
            throw new Error('Gemini API key not configured');
        }
        
        debugLog(env, 'Making Gemini text request', { model: params.model });

        const genAI = new GoogleGenAI({ apiKey: env.GEMINI_API_KEY });
        
        // Convert messages to Gemini format
        const conversationMessages = params.messages.filter((msg: ChatMessage) => msg.role !== 'system');
        
        const history = conversationMessages.map((msg: ChatMessage) => ({
            role: msg.role === 'assistant' ? 'model' : 'user',
            parts: [{ text: msg.content }]
        }));

        if (params.stream) {
            const { readable, writable } = new TransformStream();
            const writer = writable.getWriter();
            const encoder = new TextEncoder();

            const streamResponse = async () => {
                try {
                    const stream = await genAI.models.generateContentStream({
                        model: params.model,
                        contents: history
                    });

                    let fullText = '';
                    for await (const chunk of stream) {
                        const chunkText = chunk.text;
                        if (chunkText) {
                            fullText += chunkText;
                            
                            // Create OpenAI-compatible streaming response
                            const delta = {
                                id: `chatcmpl-${generateId()}`,
                                object: 'chat.completion.chunk',
                                created: Math.floor(Date.now() / 1000),
                                model: params.model,
                                choices: [{
                                    index: 0,
                                    delta: { content: chunkText },
                                    finish_reason: null
                                }]
                            };
                            
                            await writer.write(encoder.encode(`data: ${JSON.stringify(delta)}\n\n`));
                        }
                    }

                    // Send final chunk
                    const finalChunk = {
                        id: `chatcmpl-${generateId()}`,
                        object: 'chat.completion.chunk',
                        created: Math.floor(Date.now() / 1000),
                        model: params.model,
                        choices: [{
                            index: 0,
                            delta: {},
                            finish_reason: 'stop'
                        }]
                    };
                    
                    await writer.write(encoder.encode(`data: ${JSON.stringify(finalChunk)}\n\n`));
                    await writer.write(encoder.encode('data: [DONE]\n\n'));
                } catch (error) {
                    errorLog('Gemini streaming error', error);
                    const errorChunk = `data: ${JSON.stringify({ error: { message: (error as Error).message, type: 'server_error' } })}\n\n`;
                    await writer.write(encoder.encode(errorChunk));
                } finally {
                    writer.close();
                }
            };

            streamResponse();
            return new Response(readable, {
                headers: { 'Content-Type': 'text/event-stream', ...corsHeaders }
            });
        } else {
            const result = await genAI.models.generateContent({
                model: params.model,
                contents: history
            });
            const text = result.text || '';

            // Save memory context
            await saveMemoryContext(params.messages, text, params.memory_keyword, env);

            const openaiResponse: ChatCompletion = {
                id: `chatcmpl-${generateId()}`,
                object: 'chat.completion',
                created: Math.floor(Date.now() / 1000),
                model: params.model,
                choices: [{ 
                    index: 0, 
                    message: { role: 'assistant', content: text, refusal: null }, 
                    finish_reason: 'stop',
                    logprobs: null
                }],
                usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 } // Usage not provided by Gemini
            };

            return new Response(JSON.stringify(openaiResponse), {
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }
    } catch (error) {
        errorLog('Gemini text request failed', error);
        const errorResponse = {
            error: {
                message: 'Gemini API error',
                type: 'api_error',
                details: error instanceof Error ? error.message : 'Unknown Gemini error',
                request_id: generateId()
            }
        };
        return new Response(JSON.stringify(errorResponse), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

async function handleCloudflareTextRequest(params: any, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        debugLog(env, `Making Cloudflare text request`, { model: params.model, type: params.modelType });
        const convertedMessages = convertMessages(params.messages, 'cloudflare', params.modelType);
    
        let aiRequestPayload: any;
        if (params.modelType === 'llama4') {
            aiRequestPayload = { 
                messages: Array.isArray(convertedMessages) ? convertedMessages : params.messages, 
                stream: params.stream 
            };
        } else {
            aiRequestPayload = { 
                input: (convertedMessages as {input: string}).input, 
                stream: params.stream 
            };
        }
        
        const responseStream: ReadableStream = await env.AI.run(params.model, aiRequestPayload);

        if (params.stream) {
            const { readable, writable } = new TransformStream();
            const writer = writable.getWriter();
            const encoder = new TextEncoder();

            const streamResponse = async () => {
                const reader = responseStream.getReader();
                try {
                    while (true) {
                        const { done, value } = await reader.read();
                        if (done) break;
                        // The value from env.AI.run stream is already a server-sent event string
                        // We just need to pass it through.
                        await writer.write(value);
                    }
                    const finalChunk = `data: [DONE]\n\n`;
                    await writer.write(encoder.encode(finalChunk));
                } catch (error) {
                    errorLog('Cloudflare streaming error', error);
                    const errorChunk = `data: ${JSON.stringify({ error: { message: (error as Error).message, type: 'server_error' } })}\n\n`;
                    await writer.write(encoder.encode(errorChunk));
                } finally {
                    writer.close();
                }
            };

            streamResponse();
            return new Response(readable, {
                headers: { 'Content-Type': 'text/event-stream', ...corsHeaders }
            });
        } else {
            const reader = responseStream.getReader();
            const chunks: Uint8Array[] = [];
            
            while (true) {
                const { done, value } = await reader.read();
                if (done) break;
                chunks.push(value);
            }
            
            const fullResponse = new TextDecoder().decode(new Uint8Array(chunks.reduce((acc, chunk) => new Uint8Array([...acc, ...chunk]), new Uint8Array(0))));
            
            // Save memory context
            await saveMemoryContext(params.messages, fullResponse, params.memory_keyword, env);
            
            return new Response(fullResponse, {
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }
    } catch (error) {
        errorLog('Cloudflare text request failed', error);
        const errorResponse = {
            error: {
                message: 'Cloudflare AI error',
                type: 'api_error',
                details: error instanceof Error ? error.message : 'Unknown Cloudflare AI error',
                request_id: generateId()
            }
        };
        return new Response(JSON.stringify(errorResponse), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

async function handleOpenAIRequest(params: any, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        if (!env.OPENAI_API_KEY) {
            throw new Error('OpenAI API key not configured');
        }
        
        debugLog(env, 'Making OpenAI API request', { model: params.model });
        
        const openai = new OpenAI({ apiKey: env.OPENAI_API_KEY });
        
        // Prepare request parameters with structured response support
        const requestParams = { ...params };
        
        // Ensure response_format is properly structured for OpenAI
        if (params.response_format && shouldUseStructuredResponse(params.model, 'openai', params.response_format)) {
            if (params.response_format.type === 'json_object') {
                requestParams.response_format = { type: 'json_object' };
            } else if (params.response_format.type === 'json_schema' && params.response_format.schema) {
                // Ensure the schema has additionalProperties: false for OpenAI
                const schema = addAdditionalPropertiesFalse(params.response_format.schema);
                
                debugLog(env, 'OpenAI structured response schema', { 
                    original: params.response_format.schema, 
                    processed: schema 
                });
                
                requestParams.response_format = {
                    type: 'json_schema',
                    json_schema: {
                        name: 'response_schema',
                        strict: true,
                        schema: schema
                    }
                };
            }
        }
        
        const completion = await openai.chat.completions.create(requestParams);

        if (params.stream) {
            // For streaming, return the completion as a readable stream
            return new Response(completion as any, {
                headers: { 'Content-Type': 'text/event-stream', ...corsHeaders }
            });
        } else {
            return new Response(JSON.stringify(completion), {
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }
    } catch (error) {
        errorLog('OpenAI request failed', error);
        const errorResponse = {
            error: {
                message: 'OpenAI API error',
                type: 'api_error',
                details: error instanceof Error ? error.message : 'Unknown OpenAI error',
                request_id: generateId()
            }
        };
        return new Response(JSON.stringify(errorResponse), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

async function handleGeminiRequest(params: any, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        if (!env.GEMINI_API_KEY) {
            throw new Error('Gemini API key not configured');
        }
        
        debugLog(env, 'Making Gemini API request', { model: params.model });

        const genAI = new GoogleGenAI({ apiKey: env.GEMINI_API_KEY });
        
        // Convert messages to Gemini format
        const conversationMessages = params.messages.filter((msg: ChatMessage) => msg.role !== 'system');
        
        const history = conversationMessages.map((msg: ChatMessage) => ({
            role: msg.role === 'assistant' ? 'model' : 'user',
            parts: [{ text: msg.content }]
        }));

        // Prepare config for structured response if needed
        let config: any = {};
        if (params.response_format && shouldUseStructuredResponse(params.model, 'gemini', params.response_format)) {
            if (params.response_format.type === 'json_object') {
                config.responseMimeType = 'application/json';
            } else if (params.response_format.type === 'json_schema' && params.response_format.schema) {
                config.responseMimeType = 'application/json';
                config.responseSchema = convertOpenAISchemaToGemini(params.response_format.schema);
            }
        }

        if (params.stream) {
            const { readable, writable } = new TransformStream();
            const writer = writable.getWriter();
            const encoder = new TextEncoder();

            const streamResponse = async () => {
                try {
                    const stream = await genAI.models.generateContentStream({
                        model: params.model,
                        contents: history,
                        config: Object.keys(config).length > 0 ? config : undefined
                    });

                    let fullText = '';
                    for await (const chunk of stream) {
                        const chunkText = chunk.text;
                        if (chunkText) {
                            fullText += chunkText;
                            
                            // Create OpenAI-compatible streaming response
                            const delta = {
                                id: `chatcmpl-${generateId()}`,
                                object: 'chat.completion.chunk',
                                created: Math.floor(Date.now() / 1000),
                                model: params.model,
                                choices: [{
                                    index: 0,
                                    delta: { content: chunkText },
                                    finish_reason: null
                                }]
                            };
                            
                            await writer.write(encoder.encode(`data: ${JSON.stringify(delta)}\n\n`));
                        }
                    }

                    // Send final chunk
                    const finalChunk = {
                        id: `chatcmpl-${generateId()}`,
                        object: 'chat.completion.chunk',
                        created: Math.floor(Date.now() / 1000),
                        model: params.model,
                        choices: [{
                            index: 0,
                            delta: {},
                            finish_reason: 'stop'
                        }]
                    };
                    
                    await writer.write(encoder.encode(`data: ${JSON.stringify(finalChunk)}\n\n`));
                    await writer.write(encoder.encode('data: [DONE]\n\n'));
                } catch (error) {
                    errorLog('Gemini streaming error', error);
                    const errorChunk = `data: ${JSON.stringify({ error: { message: (error as Error).message, type: 'server_error' } })}\n\n`;
                    await writer.write(encoder.encode(errorChunk));
                } finally {
                    writer.close();
                }
            };

            streamResponse();
            return new Response(readable, {
                headers: { 'Content-Type': 'text/event-stream', ...corsHeaders }
            });
        } else {
            const result = await genAI.models.generateContent({
                model: params.model,
                contents: history,
                config: Object.keys(config).length > 0 ? config : undefined
            });
            const text = result.text || '';

            const openaiResponse: ChatCompletion = {
                id: `chatcmpl-${generateId()}`,
                object: 'chat.completion',
                created: Math.floor(Date.now() / 1000),
                model: params.model,
                choices: [{ 
                    index: 0, 
                    message: { role: 'assistant', content: text, refusal: null }, 
                    finish_reason: 'stop',
                    logprobs: null
                }],
                usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 } // Usage not provided by Gemini
            };
            return new Response(JSON.stringify(openaiResponse), {
                headers: { 'Content-Type': 'application/json', ...corsHeaders }
            });
        }
    } catch (error) {
        errorLog('Gemini request failed', error);
        const errorResponse = {
            error: {
                message: 'Gemini API error',
                type: 'api_error',
                details: error instanceof Error ? error.message : 'Unknown Gemini error',
                request_id: generateId()
            }
        };
        return new Response(JSON.stringify(errorResponse), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

async function handleCloudflareRequest(params: any, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        debugLog(env, `Making Cloudflare AI request`, { model: params.model, type: params.modelType });
        const convertedMessages = convertMessages(params.messages, 'cloudflare', params.modelType);
    
        let aiRequestPayload: any;
        if (params.modelType === 'llama4') {
            aiRequestPayload = { 
                messages: Array.isArray(convertedMessages) ? convertedMessages : params.messages, 
                stream: params.stream 
            };
            
            // Add structured output support for advanced models
            if (params.response_format && shouldUseStructuredResponse(params.model, 'cloudflare', params.response_format)) {
                if (params.response_format.type === 'json_object') {
                    aiRequestPayload.response_format = { type: 'json_object' };
                } else if (params.response_format.type === 'json_schema' && params.response_format.schema) {
                    aiRequestPayload.response_format = { 
                        type: 'json_schema',
                        json_schema: params.response_format.schema
                    };
                }
            }
        } else {
            aiRequestPayload = { 
                input: (convertedMessages as {input: string}).input, 
                stream: params.stream 
            };
        }
    
    const responseStream: ReadableStream = await env.AI.run(params.model, aiRequestPayload);

    if (params.stream) {
        const { readable, writable } = new TransformStream();
        const writer = writable.getWriter();
        const encoder = new TextEncoder();

        const streamResponse = async () => {
            const reader = responseStream.getReader();
            try {
                while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;
                    // The value from env.AI.run stream is already a server-sent event string
                    // We just need to pass it through.
                    await writer.write(value);
                }
                const finalChunk = `data: [DONE]\n\n`;
                await writer.write(encoder.encode(finalChunk));
            } catch (error) {
                errorLog('Cloudflare streaming error', error);
                const errorChunk = `data: ${JSON.stringify({ error: { message: (error as Error).message, type: 'server_error' } })}\n\n`;
                await writer.write(encoder.encode(errorChunk));
            } finally {
                writer.close();
                reader.releaseLock();
            }
        };

        streamResponse();
        return new Response(readable, {
            headers: { 'Content-Type': 'text/event-stream', ...corsHeaders }
        });
    } else {
        const response: any = await responseStream;
        let content = response.response || response.result || response.text || (typeof response === 'string' ? response : JSON.stringify(response));

        const openaiResponse: ChatCompletion = {
            id: `chatcmpl-${generateId()}`,
            object: 'chat.completion',
            created: Math.floor(Date.now() / 1000),
            model: params.originalModel,
            choices: [{ 
                index: 0, 
                message: { role: 'assistant', content, refusal: null }, 
                finish_reason: 'stop',
                logprobs: null
            }],
            usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 } // Usage not provided by CF
        };

        return new Response(JSON.stringify(openaiResponse), {
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
    } catch (error) {
        errorLog('Cloudflare AI request failed', error);
        const errorResponse = {
            error: {
                message: 'Cloudflare AI error',
                type: 'api_error',
                details: error instanceof Error ? error.message : 'Unknown Cloudflare AI error',
                request_id: generateId()
            }
        };
        return new Response(JSON.stringify(errorResponse), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}


// --- Model & Message Handling ---

function detectProvider(model: string, env: Env): Provider {
    const m = model.toLowerCase();
    if (m.startsWith('@cf/')) return 'cloudflare';
    if (m.includes('gpt')) return 'openai';
    if (m.includes('gemini') || m.includes('bison')) return 'gemini';
    if (env.OPENAI_API_KEY && (m === 'gpt-4' || m === 'gpt-3.5-turbo')) return 'openai';
    return 'cloudflare';
}

function getModelType(model: string, provider: Provider): ModelType {
    if (provider === 'cloudflare') {
        const m = model.toLowerCase();
        if (m.includes('llama-4')) return 'llama4';
        if (m.includes('llama')) return 'llama';
        if (m.includes('openai') || m.includes('gpt-oss')) return 'openai';
    }
    if (provider === 'openai') return 'openai';
    if (provider === 'gemini') return 'gemini';
    return 'input';
}

/**
 * Determines if a model should use structured response handling
 */
function shouldUseStructuredResponse(model: string, provider: Provider, responseFormat?: any): boolean {
    // For Gemini and OpenAI, always use structured response if responseFormat is provided
    if (provider === 'gemini' || provider === 'openai') {
        return responseFormat && responseFormat.type !== 'text';
    }
    
    // For Cloudflare AI, only use structured response for specific models that support it
    if (provider === 'cloudflare') {
        const cloudflareStructuredModels = [
            '@cf/meta/llama-4-scout-17b-16e-instruct',
            '@cf/meta/llama-3-8b-instruct',
            '@cf/meta/llama-3-70b-instruct',
            
        ];
        
        return cloudflareStructuredModels.includes(model) && 
               responseFormat && 
               responseFormat.type !== 'text';
    }
    
    return false;
}

/**
 * Recursively adds additionalProperties: false to all objects in a schema for OpenAI compatibility
 */
function addAdditionalPropertiesFalse(schema: any): any {
    if (!schema || typeof schema !== 'object') {
        return schema;
    }
    
    const result = { ...schema };
    
    // Add additionalProperties: false to objects
    if (result.type === 'object') {
        result.additionalProperties = false;
        
        // Ensure required array only contains keys that exist in properties
        if (result.required && result.properties) {
            const propertyKeys = Object.keys(result.properties);
            result.required = result.required.filter((key: string) => propertyKeys.includes(key));
        }
    }
    
    // Recursively process properties
    if (result.properties) {
        result.properties = {};
        for (const [key, value] of Object.entries(result.properties)) {
            result.properties[key] = addAdditionalPropertiesFalse(value);
        }
    }
    
    // Recursively process items for arrays
    if (result.items) {
        result.items = addAdditionalPropertiesFalse(result.items);
    }
    
    return result;
}

/**
 * Converts OpenAI JSON schema format to Gemini schema format
 */
function convertOpenAISchemaToGemini(openaiSchema: any): any {
    if (!openaiSchema || !openaiSchema.schema) {
        return {};
    }
    
    const schema = openaiSchema.schema;
    
    // Convert OpenAI schema to Gemini Type format
    function convertType(type: string): any {
        switch (type) {
            case 'string':
                return { type: 'STRING' };
            case 'number':
                return { type: 'NUMBER' };
            case 'integer':
                return { type: 'INTEGER' };
            case 'boolean':
                return { type: 'BOOLEAN' };
            case 'array':
                return { type: 'ARRAY' };
            case 'object':
                return { type: 'OBJECT' };
            default:
                return { type: 'STRING' };
        }
    }
    
    function convertSchema(obj: any): any {
        if (!obj) return {};
        
        const result: any = {};
        
        if (obj.type) {
            const converted = convertType(obj.type);
            Object.assign(result, converted);
        }
        
        if (obj.properties) {
            result.properties = {};
            for (const [key, value] of Object.entries(obj.properties)) {
                result.properties[key] = convertSchema(value);
            }
        }
        
        if (obj.items) {
            result.items = convertSchema(obj.items);
        }
        
        if (obj.required) {
            result.required = obj.required;
        }
        
        if (obj.description) {
            result.description = obj.description;
        }
        
        return result;
    }
    
    return convertSchema(schema);
}

function mapModelName(model: string, provider: Provider, env: Env): string {
    if (provider === 'cloudflare') {
        const map: Record<string, string> = {
            'gpt-4': env.DEFAULT_MODEL || '@cf/meta/llama-4-scout-17b-16e-instruct',
            'gpt-4-turbo': env.DEFAULT_MODEL || '@cf/meta/llama-4-scout-17b-16e-instruct',
            'gpt-4o': env.DEFAULT_MODEL || '@cf/meta/llama-4-scout-17b-16e-instruct',
            'gpt-3.5-turbo': env.BACKUP_MODEL || '@cf/openai/gpt-oss-120b',
        };
        return map[model] || model;
    }
    if (provider === 'gemini') {
        const map: Record<string, string> = {
            'gpt-4': 'gemini-1.5-pro',
            'gpt-4o': 'gemini-1.5-pro',
            'gpt-3.5-turbo': 'gemini-1.5-flash',
        };
        return map[model] || model;
    }
    return model;
}

function convertMessages(messages: ChatMessage[], provider: Provider, modelType: ModelType): ChatMessage[] | { input: string } {
    if (provider === 'openai' || provider === 'gemini' || (provider === 'cloudflare' && modelType === 'llama4')) {
        return messages;
    }

    if (provider === 'cloudflare') {
        if (modelType === 'llama') {
            const input = messages.map(msg => `<|start_header_id|>${msg.role}<|end_header_id|>\n\n${msg.content}<|eot_id|>`).join('\n');
            return { input };
        }
    }
    
    const input = messages.map(msg => `${msg.role}: ${msg.content}`).join('\n');
    return { input };
}


// --- Other Endpoints ---

async function handleModelsRequest(_request: Request, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    try {
        const [cloudflareModels, openaiModels, geminiModels] = await Promise.all([
            getCloudflareModels(env),
            getOpenAIModels(env),
            getGeminiModels(env)
        ]);

        let allModels: ApiModel[] = [...cloudflareModels, ...openaiModels, ...geminiModels];

        const compatibilityModels: ApiModel[] = [
            { id: 'gpt-4', object: 'model', owner: 'openai-compat' },
            { id: 'gpt-4o', object: 'model', owner: 'openai-compat' },
            { id: 'gpt-3.5-turbo', object: 'model', owner: 'openai-compat' },
        ];

        allModels = [...compatibilityModels, ...allModels];
        const uniqueModels = Array.from(new Map(allModels.map(m => [m.id, m])).values());

        return new Response(JSON.stringify({ data: uniqueModels, object: 'list' }), {
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    } catch (error) {
        errorLog('Error fetching models', error);
        return new Response(JSON.stringify({ error: { message: 'Failed to fetch models', type: 'server_error' } }), {
            status: 500,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
}

async function handleCompletions(request: Request, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    const body = await request.json() as CompletionRequestBody;
    if (!body.prompt) {
        return new Response(JSON.stringify({ error: { message: '`prompt` parameter is required', type: 'invalid_request_error' } }), {
            status: 400,
            headers: { 'Content-Type': 'application/json', ...corsHeaders }
        });
    }
    
    // Convert to chat format and reuse chat completions handler
    const chatRequest = new Request(request.url.replace('/completions', '/chat/completions'), {
        method: 'POST',
        headers: request.headers,
        body: JSON.stringify({
            model: body.model,
            messages: [{ role: 'user', content: body.prompt }],
            max_tokens: body.max_tokens,
            temperature: body.temperature,
            stream: body.stream
        })
    });
    
    return handleChatCompletions(chatRequest, env, corsHeaders);
}


// --- Model Fetching ---

async function getCloudflareModels(env: Env): Promise<ApiModel[]> {
    try {
        if (env.CORE_API && env.CORE_WORKER_API_KEY) {
            const resp = await env.CORE_API.fetch('https://core-api.hacolby.workers.dev/ai/models', {
                method: 'GET',
                headers: {
                    'X-API-Key': env.CORE_WORKER_API_KEY,
                    'Content-Type': 'application/json',
                },
            });

            if (resp.ok) {
                const data = (await resp.json()) as CloudflareAIModelsResponse;

                const models: ApiModel[] = [];
                for (const [providerName, providerModels] of Object.entries(data.providers || {})) {
                    for (const m of providerModels || []) {
                        const taskName = (m.task?.name || '').toLowerCase(); // schema has task.name
                        // be flexible: "text generation", "text-generation", "text gen", etc.
                        if (taskName.includes('text') && taskName.includes('gen')) {
                            models.push({
                                id: m.name || m.id,                       // schema: both exist; UI shows `name`
                                object: 'model',
                                owner: `cloudflare-${providerName}`,
                                created: m.created_at ? Date.parse(m.created_at) / 1000 : undefined,
                            });
                        }
                    }
                }

                if (models.length > 0) {
                    debugLog(env, `Loaded ${models.length} Cloudflare models from core API`);
                    return models;
                }
            } else {
                const t = await resp.text();
                errorLog(`Core API /ai/models ${resp.status}`, t);
            }
        }

        // Fallbacks
        debugLog(env, 'Using fallback Cloudflare models list');
        return [
            { id: '@cf/meta/llama-4-scout-17b-16e-instruct', object: 'model', owner: 'cloudflare-meta' },
            { id: '@cf/meta/llama-3-8b-instruct', object: 'model', owner: 'cloudflare-meta' },
            { id: '@cf/meta/llama-3-70b-instruct', object: 'model', owner: 'cloudflare-meta' },
            { id: '@cf/openai/gpt-oss-120b', object: 'model', owner: 'cloudflare-openai' },
            { id: '@cf/google/gemma-7b-it', object: 'model', owner: 'cloudflare-google' },
            { id: '@cf/google/gemma-2-9b-it', object: 'model', owner: 'cloudflare-google' },
            { id: '@cf/microsoft/phi-3-mini-128k-instruct', object: 'model', owner: 'cloudflare-microsoft' },
            { id: '@cf/huggingface/microsoft/DialoGPT-medium', object: 'model', owner: 'cloudflare-huggingface' },
            { id: '@cf/qwen/qwen-2.5-7b-instruct', object: 'model', owner: 'cloudflare-qwen' },
            { id: '@cf/cohere/command-r-plus', object: 'model', owner: 'cloudflare-cohere' },
        ];
    } catch (err) {
        errorLog('Failed to fetch Cloudflare models', err);
        return [
            { id: '@cf/meta/llama-4-scout-17b-16e-instruct', object: 'model', owner: 'cloudflare-meta' },
            { id: '@cf/meta/llama-3-8b-instruct', object: 'model', owner: 'cloudflare-meta' },
            { id: '@cf/openai/gpt-oss-120b', object: 'model', owner: 'cloudflare-openai' },
            { id: '@cf/google/gemma-7b-it', object: 'model', owner: 'cloudflare-google' },
            { id: '@cf/microsoft/phi-3-mini-128k-instruct', object: 'model', owner: 'cloudflare-microsoft' },
        ];
    }
}

async function getOpenAIModels(env: Env): Promise<ApiModel[]> {
    if (!env.OPENAI_API_KEY) return [];
    try {
        const openai = new OpenAI({ apiKey: env.OPENAI_API_KEY });
        const models = await openai.models.list();
        return models.data
            .filter(m => m.id.includes('gpt'))
            .map(m => ({ id: m.id, object: 'model', owner: 'openai', created: m.created }));
    } catch (error) {
        debugLog(env, 'Could not fetch OpenAI models', { error: (error as Error).message });
        return [];
    }
}

async function getGeminiModels(env: Env): Promise<ApiModel[]> {
    if (!env.GEMINI_API_KEY) return [];
    try {
        const genAI = new GoogleGenAI({ apiKey: env.GEMINI_API_KEY });
        const models = await genAI.models.list();
        if (!Array.isArray(models)) return [];
        return models
            .filter((m: any) => Array.isArray(m.supportedGenerationMethods) && m.supportedGenerationMethods.includes('generateContent'))
            .map((m: any) => ({ 
                id: typeof m.name === 'string' ? m.name.replace('models/', '') : '',
                object: 'model', 
                owner: 'google',
                created: m.createTime ? new Date(m.createTime).getTime() / 1000 : undefined
            }));
    } catch (error) {
        debugLog(env, 'Could not fetch Gemini models', { error: (error as Error).message });
        return [];
    }
}

function generateId(length = 12): string {
    return Math.random().toString(36).substring(2, 2 + length);
}

async function handleTestAPIs(_request: Request, env: Env, corsHeaders: Record<string, string>): Promise<Response> {
    const results = {
        timestamp: new Date().toISOString(),
        tests: {
            coreApi: { status: 'PENDING', message: '', models: 0, providers: 0 },
            openai: { status: 'PENDING', message: '', models: 0 },
            gemini: { status: 'PENDING', message: '', models: 0 },
            cloudflareAI: { status: 'PENDING', message: '', models: 0 }
        }
    };

    // Test Core API
    try {
        if (env.CORE_API && env.CORE_WORKER_API_KEY) {
            const resp = await env.CORE_API.fetch('https://core-api.hacolby.workers.dev/ai/models', {
                method: 'GET',
                headers: {
                    'X-API-Key': env.CORE_WORKER_API_KEY,
                    'Content-Type': 'application/json',
                },
            });

            if (resp.ok) {
                const data = await resp.json() as CloudflareAIModelsResponse;
                const totalModels = Object.values(data.providers || {}).flat().length;
                const providers = Object.keys(data.providers || {}).length;
                
                results.tests.coreApi = {
                    status: 'PASS',
                    message: `Successfully connected to Core API`,
                    models: totalModels,
                    providers: providers
                };
            } else {
                const errorText = await resp.text();
                results.tests.coreApi = {
                    status: 'FAIL',
                    message: `Core API returned ${resp.status}: ${errorText}`,
                    models: 0,
                    providers: 0
                };
            }
        } else {
            results.tests.coreApi = {
                status: 'FAIL',
                message: 'Core API not configured (missing CORE_API or CORE_WORKER_API_KEY)',
                models: 0,
                providers: 0
            };
        }
    } catch (error) {
        results.tests.coreApi = {
            status: 'FAIL',
            message: `Core API error: ${error instanceof Error ? error.message : 'Unknown error'}`,
            models: 0,
            providers: 0
        };
    }

    // Test OpenAI
    try {
        if (env.OPENAI_API_KEY) {
            const openai = new OpenAI({ apiKey: env.OPENAI_API_KEY });
            const models = await openai.models.list();
            const gptModels = models.data.filter(m => m.id.includes('gpt')).length;
            
            results.tests.openai = {
                status: 'PASS',
                message: `Successfully connected to OpenAI API`,
                models: gptModels
            };
        } else {
            results.tests.openai = {
                status: 'FAIL',
                message: 'OpenAI API key not configured',
                models: 0
            };
        }
    } catch (error) {
        results.tests.openai = {
            status: 'FAIL',
            message: `OpenAI error: ${error instanceof Error ? error.message : 'Unknown error'}`,
            models: 0
        };
    }

    // Test Gemini
    try {
        if (env.GEMINI_API_KEY) {
            const genAI = new GoogleGenAI({ apiKey: env.GEMINI_API_KEY });
            const models = await genAI.models.list();
            const textModels = Array.isArray(models) ? models.filter((m: any) => 
                Array.isArray(m.supportedGenerationMethods) && 
                m.supportedGenerationMethods.includes('generateContent')
            ).length : 0;
            
            results.tests.gemini = {
                status: 'PASS',
                message: `Successfully connected to Gemini API`,
                models: textModels
            };
        } else {
            results.tests.gemini = {
                status: 'FAIL',
                message: 'Gemini API key not configured',
                models: 0
            };
        }
    } catch (error) {
        results.tests.gemini = {
            status: 'FAIL',
            message: `Gemini error: ${error instanceof Error ? error.message : 'Unknown error'}`,
            models: 0
        };
    }

    // Test Cloudflare AI (local binding)
    try {
        if (env.AI) {
            // Test with a simple model to see if the binding works
            await env.AI.run('@cf/meta/llama-3-8b-instruct', {
                messages: [{ role: 'user', content: 'Hello' }]
            });
            
            results.tests.cloudflareAI = {
                status: 'PASS',
                message: `Successfully connected to Cloudflare AI binding`,
                models: 1 // We know at least one model works
            };
        } else {
            results.tests.cloudflareAI = {
                status: 'FAIL',
                message: 'Cloudflare AI binding not configured',
                models: 0
            };
        }
    } catch (error) {
        results.tests.cloudflareAI = {
            status: 'FAIL',
            message: `Cloudflare AI error: ${error instanceof Error ? error.message : 'Unknown error'}`,
            models: 0
        };
    }

    return new Response(JSON.stringify(results), {
        headers: { 'Content-Type': 'application/json', ...corsHeaders }
    });
}
